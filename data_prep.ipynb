{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223f14f2",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c46a3",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc76dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy import signal\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5896c4b",
   "metadata": {},
   "source": [
    "### Function for detecting the sensor errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be869fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_errors(data):\n",
    "    for i in range(data.shape[0]):\n",
    "        if (i==0 and data[i]>1000):\n",
    "            if (data[i+1]<1000):\n",
    "                data[i] == data[i+1]\n",
    "            else:\n",
    "                while (data[k]>1000):\n",
    "                    k == data.shape[0] - 1\n",
    "                    data[i] = data[k]\n",
    "        if (data[i]>1000 and i>0):\n",
    "            data[i] = data[i-1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0fc9f8",
   "metadata": {},
   "source": [
    "### Function for preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9a8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(dataset):\n",
    "    \n",
    "    dataset.index = range(0, dataset.shape[0])\n",
    "    rpc_values = np.array(dataset[['Ax.1', 'Ay.1', 'Az.1']], dtype='float')\n",
    "    \n",
    "    acceleration = np.array([np.linalg.norm(rpc_values[x, :]) for x in range(0, rpc_values.shape[0])], dtype = 'float')\n",
    "    acceleration = sensor_errors(acceleration)\n",
    "    y = np.array(dataset[['Unnamed: 69']])\n",
    "    \n",
    "    participant = pd.DataFrame()\n",
    "    participant['a'] = acceleration\n",
    "    participant['y'] = y\n",
    "    \n",
    "    mask = participant['y'] == 'upsatirs'\n",
    "    participant['y'].loc[mask] = 'upstairs'\n",
    "    \n",
    "    new_participant = participant.reindex(index=participant.index[::-1])\n",
    "    participant = pd.concat([participant,new_participant.iloc[0:1000,:]])\n",
    "    participant.index = range(participant.shape[0])\n",
    "    \n",
    "    return participant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f2a84",
   "metadata": {},
   "source": [
    "### Funtion for preprocessing the data of the left pocket for each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eba1dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_left_pocket(dataset):\n",
    "    \n",
    "    dataset.index = range(0, dataset.shape[0])\n",
    "    rpc_values = np.array(dataset[['Ax', 'Ay', 'Az']], dtype='float')\n",
    "    \n",
    "    acceleration = np.array([np.linalg.norm(rpc_values[x, :]) for x in range(0, rpc_values.shape[0])], dtype = 'float')\n",
    "    acceleration = sensor_errors(acceleration)\n",
    "    y = np.array(dataset[['Unnamed: 69']])\n",
    "    \n",
    "    participant = pd.DataFrame()\n",
    "    participant['a'] = acceleration\n",
    "    participant['y'] = y\n",
    "    \n",
    "    mask = participant['y'] == 'upsatirs'\n",
    "    participant['y'].loc[mask] = 'upstairs'\n",
    "    \n",
    "    new_participant = participant.reindex(index=participant.index[::-1])\n",
    "    participant = pd.concat([participant,new_participant.iloc[0:1000,:]])\n",
    "    participant.index = range(participant.shape[0])\n",
    "    \n",
    "    return participant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f9051",
   "metadata": {},
   "source": [
    "### Function for preprocessing the data of the wrist for each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d9d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_wrist(dataset):\n",
    "    \n",
    "    dataset.index = range(0, dataset.shape[0])\n",
    "    rpc_values = np.array(dataset[['Ax.2', 'Ay.2', 'Az.2']], dtype='float')\n",
    "    \n",
    "    acceleration = np.array([np.linalg.norm(rpc_values[x, :]) for x in range(0, rpc_values.shape[0])], dtype = 'float')\n",
    "    acceleration = sensor_errors(acceleration)\n",
    "    y = np.array(dataset[['Unnamed: 69']])\n",
    "    \n",
    "    participant = pd.DataFrame()\n",
    "    participant['a'] = acceleration\n",
    "    participant['y'] = y\n",
    "    \n",
    "    mask = participant['y'] == 'upsatirs'\n",
    "    participant['y'].loc[mask] = 'upstairs'\n",
    "    \n",
    "    new_participant = participant.reindex(index=participant.index[::-1])\n",
    "    participant = pd.concat([participant,new_participant.iloc[0:1000,:]])\n",
    "    participant.index = range(participant.shape[0])\n",
    "    \n",
    "    return participant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5bc3ef",
   "metadata": {},
   "source": [
    "### Function for returning the most common label in each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b615b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(List):\n",
    "    count = Counter(List)\n",
    "    return count.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ee916",
   "metadata": {},
   "source": [
    "### Function for the feature extraction of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e763060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(participant):\n",
    "    \n",
    "    mean_list = []\n",
    "    std_list = []\n",
    "    skew_list = []\n",
    "    max_list = []\n",
    "    min_list = []\n",
    "    min_max_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for w in range(0, participant.shape[0]-1000, 50):\n",
    "        end = w+1000\n",
    "        mean_list.append(np.mean(participant.iloc[w:end, 0]))\n",
    "        std_list.append(np.std(participant.iloc[w:end, 0]))\n",
    "        skew_list.append(skew(participant.iloc[w:end, 0]))\n",
    "        max_list.append(np.max(participant.iloc[w:end, 0]))\n",
    "        min_list.append(np.min(participant.iloc[w:end, 0]))\n",
    "        min_max_list.append(np.max(participant.iloc[w:end, 0]) - np.min(participant.iloc[w:end, 0]))\n",
    "        \n",
    "        f,p = signal.welch(participant.iloc[w:end, 0], nperseg=128)\n",
    "        \n",
    "        if (w==0):\n",
    "            n = f.shape[0]\n",
    "            welch_lists = [[] for i in range(n)]\n",
    "            \n",
    "        for t in range(0,n):\n",
    "            welch_lists[t].append(p[t])\n",
    "        \n",
    "        y_list.append(most_common(participant['y'][w:end]))\n",
    "        \n",
    "    df['Mean'] = mean_list\n",
    "    df['Std'] = std_list\n",
    "    df['Skew'] = skew_list\n",
    "    df['Max'] = max_list\n",
    "    df['Min'] = min_list\n",
    "    df['Min-Max'] = min_max_list\n",
    "    \n",
    "    for k in range(0,n):\n",
    "        df['Accwelch'+str(k)] = welch_lists[k]\n",
    "    \n",
    "    df['y'] = y_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73936e9e",
   "metadata": {},
   "source": [
    "### Code Snippet for creating the train and test set for each fold of the LOSO Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c11a7b7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\nos.makedirs('train_test_dataset', exist_ok=True)\\n\\nfilenames = []\\n    \\nfor i in range(0,10):\\n    filenames.append('dataset/Participant_' +str(i+1)+ '.csv')\\n    \\nfor j in range(len(filenames)):\\n    current_participant = pd.read_csv(filenames[j], header=1)\\n    train = pd.DataFrame()\\n        \\n    for t in range(len(filenames)):\\n        if (t==j):\\n            continue\\n        train = pd.concat([train, pd.read_csv(filenames[t], header=1)])\\n        \\n    participant = data_preprocessing(current_participant)\\n    train = data_preprocessing(train)\\n        \\n    test = feature_extraction(participant)\\n    train_features = feature_extraction(train)\\n        \\n    X_train = train_features.drop('y', axis=1)\\n    X_train.to_csv('train_test_dataset/X_train_fold_' +str(j+1)+ '.csv', index=False)\\n    X_test = test.drop('y', axis=1)\\n    X_test.to_csv('train_test_dataset/X_test_fold_' +str(j+1)+ '.csv', index=False)\\n    y_train = train_features['y']\\n    y_train.to_csv('train_test_dataset/y_train_fold_' +str(j+1)+ '.csv', index=False)\\n    y_test = test['y']\\n    y_test.to_csv('train_test_dataset/y_test_fold_' +str(j+1)+ '.csv', index=False)\\n    \\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs('train_test_dataset', exist_ok=True)\n",
    "\n",
    "filenames = []\n",
    "    \n",
    "for i in range(0,10):\n",
    "    filenames.append('dataset/Participant_' +str(i+1)+ '.csv')\n",
    "    \n",
    "for j in range(len(filenames)):\n",
    "    current_participant = pd.read_csv(filenames[j], header=1)\n",
    "    train = pd.DataFrame()\n",
    "        \n",
    "    for t in range(len(filenames)):\n",
    "        if (t==j):\n",
    "            continue\n",
    "        train = pd.concat([train, pd.read_csv(filenames[t], header=1)])\n",
    "        \n",
    "    participant = data_preprocessing(current_participant)\n",
    "    train = data_preprocessing(train)\n",
    "        \n",
    "    test = feature_extraction(participant)\n",
    "    train_features = feature_extraction(train)\n",
    "        \n",
    "    X_train = train_features.drop('y', axis=1)\n",
    "    X_train.to_csv('train_test_dataset/X_train_fold_' +str(j+1)+ '.csv', index=False)\n",
    "    X_test = test.drop('y', axis=1)\n",
    "    X_test.to_csv('train_test_dataset/X_test_fold_' +str(j+1)+ '.csv', index=False)\n",
    "    y_train = train_features['y']\n",
    "    y_train.to_csv('train_test_dataset/y_train_fold_' +str(j+1)+ '.csv', index=False)\n",
    "    y_test = test['y']\n",
    "    y_test.to_csv('train_test_dataset/y_test_fold_' +str(j+1)+ '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437964a7",
   "metadata": {},
   "source": [
    "### Code Snippet for joining the 'Standing' and 'Sitting' label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5cbe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('regrouped_dataset', exist_ok=True)\n",
    "\n",
    "for i in range(0,10):\n",
    "    y_train = pd.read_csv('train_test_dataset/y_train_fold_1.csv')\n",
    "    y_test = pd.read_csv('train_test_dataset/y_test_fold_1.csv')\n",
    "\n",
    "    for t in range(y_train.shape[0]):\n",
    "        if y_train['y'][t] == 'standing':\n",
    "            y_train['y'][t] = 'standing_sitting'\n",
    "        elif y_train['y'][t] == 'sitting':\n",
    "            y_train['y'][t] = 'standing_sitting'\n",
    "    for t in range(y_test.shape[0]):\n",
    "        if y_test['y'][t] == 'standing':\n",
    "            y_test['y'][t] = 'standing_sitting'\n",
    "        elif y_test['y'][t] == 'sitting':\n",
    "            y_test['y'][t] = 'standing_sitting'\n",
    "            \n",
    "    y_train.to_csv('regrouped_dataset/y_train_' +str(i+1)+ '_regrouped.csv', index=False)\n",
    "    y_test.to_csv('regrouped_dataset/y_test_' +str(i+1)+ '_regrouped.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4eaa9a",
   "metadata": {},
   "source": [
    "### Code Snippet for creating the train set with all participants and the test set with left pocket and wrist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6b868",
   "metadata": {},
   "source": [
    "We preprocessed and extracted the features for each participant using the left pocket and the wrist columns. After training the model, you can test it for each of the 10 participants using either the left pocket values or the wrist values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da80d1f3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('10_Subject_Dataset', exist_ok=True)\n",
    "os.makedirs('dataset/Left_Pocket_Dataset', exist_ok=True)\n",
    "os.makedirs('dataset/Wrsit_Dataset', exist_ok=True)\n",
    "\n",
    "filenames=[]\n",
    "\n",
    "for i in range(10):\n",
    "    filenames.append('dataset/Participant_' +str(i+1)+ '.csv')\n",
    "    \n",
    "data = pd.DataFrame()\n",
    "    \n",
    "for t in range(len(filenames)):\n",
    "    data = pd.concat([data, pd.read_csv(filenames[t], header=1)])\n",
    "\n",
    "dataset = data_preprocessing(data)\n",
    "data_features = feature_extraction(dataset)\n",
    "\n",
    "X_train = data_features.drop('y', axis=1)\n",
    "y_train = data_features['y']\n",
    "\n",
    "X_train.to_csv('10_Subject_Dataset/X_train.csv', index=False)\n",
    "y_train.to_csv('10_Subject_Dataset/y_train.csv', index=False)\n",
    "\n",
    "for i in range(10):\n",
    "    participant_l = pd.read_csv(filenames[i], header=1)\n",
    "    participant_w = pd.read_csv(filenames[i], header=1)\n",
    "    \n",
    "    participant_l = data_preprocessing_left_pocket(participant_l)\n",
    "    participant_w = data_preprocessing_wrist(participant_w)\n",
    "    \n",
    "    participant_l_features = feature_extraction(participant_l)\n",
    "    participant_w_features = feature_extraction(participant_w)\n",
    "    \n",
    "    participant_l_features.to_csv('dataset/Left_Pocket_Dataset/Participant_' +str(i+1)+ '_left.csv', index=False)\n",
    "    participant_w_features.to_csv('dataset/Wrsit_Dataset/Participant_' +str(i+1)+ '_wrist.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
